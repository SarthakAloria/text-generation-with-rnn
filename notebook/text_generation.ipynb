{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Text Genrator\n",
        "\n",
        "> An experimental neural text generator that uses Recurrent Neural Networks (RNNs) to craft original poems and play-style scripts. Trained on existing literary works, the model learns rhythmic and structural patterns of language, enabling it to generate creative, coherent text one character at a time. It explores how simple sequence models can emulate the cadence of human writing — blending art and machine learning in a single experiment.\n",
        "\n",
        "This is based on: [TensorFlow Text Genration](https://www.tensorflow.org/text/tutorials/text_generation)"
      ],
      "metadata": {
        "id": "B9_OA0aGGprI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "1iISQGCDQebX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aAxbbkHHGlba"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "> This project uses the **Shakespeare dataset**, a publicly available text corpus containing all of William Shakespeare's plays and poems. It's commonly used for character-level text generation tasks because it contains diverse sentence structures, emotions, and dialogue formats — making it ideal for learning long-term language dependencies with RNNs."
      ],
      "metadata": {
        "id": "xRNjmbcyRIxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECXb5eb_RKNb",
        "outputId": "419a22f5-9fd0-43fa-d3f5-c1c4dde5e3a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Your Own Data\n",
        "\n",
        "> To load your own data you'll need to upload a file from the dialog below. Then you'll need to follow the steps from above but load in this new file instead."
      ],
      "metadata": {
        "id": "70K4aSnCSGOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    path_to_file = list(uploaded.keys())[0]\n",
        "    print(f\"✅ Uploaded: {path_to_file}\")\n",
        "else:\n",
        "    print(\"⚠️ No file uploaded — using default dataset instead.\")\n",
        "    path_to_file = tf.keras.utils.get_file(\n",
        "        'shakespeare.txt',\n",
        "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "F7JoGCPhR2EN",
        "outputId": "b00a742b-8725-4eae-d3d6-e7f2428fda19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d86ff6c-6a2a-491d-b6db-fff326355316\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d86ff6c-6a2a-491d-b6db-fff326355316\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No file uploaded — using default dataset instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Content of Data\n",
        "\n",
        "> Let's look at the content of file"
      ],
      "metadata": {
        "id": "Yk834gQJV3Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW3Qbtf7So3f",
        "outputId": "65d9957a-73da-47eb-9d77-febac48b5128"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Take a look at the first 250 characters in text"
      ],
      "metadata": {
        "id": "YecD0_rkWYQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auyd5nMMWMPo",
        "outputId": "7e75d7a9-ba65-4767-cc87-bb8e7f47ce26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The unique characters in the file"
      ],
      "metadata": {
        "id": "Pr1WGyqxWo6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0PSLjMTWaRp",
        "outputId": "c90ef2dd-508c-48f7-c602-8c518daa8174"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing The Data\n",
        "\n",
        "> Before training the RNN model, we need to process the raw text data into a numerical form that the model can understand. This step involves **encoding** the text into numbers and later **decoding** the model’s predictions back into text."
      ],
      "metadata": {
        "id": "JH6SKF6UZjv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding\n",
        "\n",
        "> Encoding converts text (characters or words) into numerical representations. Each unique token is mapped to an integer index, forming sequences that can be fed into the neural network."
      ],
      "metadata": {
        "id": "xGoTWxpwW1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "jEw3ubb9WrcC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Let's look at how part of our text is encoded"
      ],
      "metadata": {
        "id": "0c0Z-fRDYlad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pws3YzPfYjck",
        "outputId": "7203545f-66e1-438a-f2a3-7d403cadfb50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoding\n",
        "\n",
        "> Decoding reverses the process — it converts numeric predictions from the model back into human-readable text."
      ],
      "metadata": {
        "id": "WW0HS-XkZxoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_otXKRuuoKV",
        "outputId": "6566c9dd-3177-42e0-fa4e-1cd9276f6fd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Training Examples\n",
        "\n",
        "> To train the model effectively, the text data must be divided into shorter sequences that serve as individual training samples.\n",
        "Each training example consists of an input sequence and a corresponding target sequence of equal length *(seq_length)*. The target sequence is simply the input shifted one character to the right. For example:  \n",
        "`input: Hell | output: ello`\n",
        "\n",
        "The process begins by generating a continuous stream of characters from the text data, which can then be segmented into these input-target pairs."
      ],
      "metadata": {
        "id": "3CAuvyWzxg04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100 # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "0Nhm1_eBxHEo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Using Batch method to turn the strean of characters into batches of desired length."
      ],
      "metadata": {
        "id": "jcVFTtcA0pXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "ZPAo2oAQ0hHO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Spliting the sequences of length of 101 and split them into input and output."
      ],
      "metadata": {
        "id": "Dyv9ML_o-v0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk): # for the example hello\n",
        "  input_text = chunk[:-1] # hell\n",
        "  target_text = chunk[1:] # ello\n",
        "  return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "f7W6BG5x05L-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Example"
      ],
      "metadata": {
        "id": "Yp2Or5fOKZEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkQ-YwgCarC",
        "outputId": "0df00e2a-9f2c-4779-dae9-7c27284829b3",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Training Batches"
      ],
      "metadata": {
        "id": "4b4g_kG4KU_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "#  (TF data is designed to work with possibly infinite sequences,\n",
        "#   so it doesn't attempt to shuffle the entire sequence in memory.\n",
        "#   Instead, it maintains a buffer in which it shuffles elements.)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOJINYi_HW6e",
        "outputId": "3fb1d249-edb2-4ea7-cfea-04821455781f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model\n",
        "\n",
        "> This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).  \n",
        "This model has three layers:\n",
        ">\n",
        "> - `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        ">\n",
        "> - `tf.keras.layers.LSTM` (or `GRU`): Processes sequences of embeddings. Here, we use `rnn_units` to define the hidden state size. Setting `return_sequences=True` ensures the output maintains the sequence length, and `stateful=True` allows the model to carry hidden states across batches.\n",
        ">\n",
        "> - `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ],
      "metadata": {
        "id": "P-IkJGsDLuVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "#   model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "#                               batch_input_size=[batch_size, None]),\n",
        "#     tf.keras.layers.LSTM(rnn_units,\n",
        "#                          return_sequences=True,\n",
        "#                          stateful=True,\n",
        "#                          recurrent_initializer='glorot_uniform'),\n",
        "#     tf.keras.layers.Dense(vocab_size)\n",
        "#   ])\n",
        "#   return model\n",
        "\n",
        "# model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "hvacylznLrbj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Stateless Model"
      ],
      "metadata": {
        "id": "8Eeob7vAif-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_model_stateless(vocab_size, embedding_dim, rnn_units):\n",
        "#     return tf.keras.Sequential([\n",
        "#         # Let the model infer batch size; variable-length sequences allowed via (None,)\n",
        "#         tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(None,)),\n",
        "#         tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=False,\n",
        "#                              recurrent_initializer='glorot_uniform'),\n",
        "#         tf.keras.layers.Dense(vocab_size)\n",
        "#     ])\n",
        "\n",
        "# model = build_model_stateless(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "K67RVE0fX88r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Statefull model with 64 `batch_size`"
      ],
      "metadata": {
        "id": "UvWFCiv9ioVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    # Input layer with fixed batch size\n",
        "    inputs = tf.keras.Input(batch_shape=(batch_size, None), dtype=tf.int32)\n",
        "\n",
        "    # Embedding layer (does NOT fix batch size; it inherits from Input)\n",
        "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "    # Stateful LSTM layer\n",
        "    x = tf.keras.layers.LSTM(\n",
        "        units=rnn_units,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "    )(x)\n",
        "\n",
        "    # Dense output layer\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Build the model\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "bbovPFaMfLH6",
        "outputId": "fd473c7a-2306-47c5-f5dd-f7e207642f83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m16,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m5,246,976\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │        \u001b[38;5;34m66,625\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Loss Function\n",
        "> The loss function evaluates how closely the model's predictions match the target data. For a character-level RNN, each timestep predicts a character ID, so `sparse_categorical_crossentropy` is typically used. This function computes the difference between the predicted probability distribution and the true character indices, guiding the model to improve during training."
      ],
      "metadata": {
        "id": "fB0Y94qqcoGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "    # shape: (BATCH_SIZE, sequence_length)\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4uc2JlDdw2r",
        "outputId": "b7090589-889b-40ba-d03f-ed811733a8a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the prediction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DmoG9UiffmOL",
        "outputId": "1b50deae-a939-4587-b7a4-d599d2b54e43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.8967954e-04 -4.8379302e-03  4.4216500e-05 ... -2.6844705e-03\n",
            "   -3.8732965e-03 -1.5947804e-03]\n",
            "  [ 3.5951680e-03 -4.5101284e-03 -1.0560903e-03 ... -2.5638705e-03\n",
            "   -6.3740410e-04 -2.1776652e-03]\n",
            "  [ 6.3097887e-03 -3.9878706e-03 -6.6788640e-04 ... -8.4625802e-04\n",
            "    1.4776374e-03  5.7361205e-03]\n",
            "  ...\n",
            "  [-1.0655216e-03 -1.1174589e-02  1.1930048e-02 ...  7.6719783e-03\n",
            "   -1.3978775e-02  3.3099328e-03]\n",
            "  [ 2.5860486e-03 -7.2288392e-03  8.2859602e-03 ...  8.6672632e-03\n",
            "   -1.0147412e-02  7.6457328e-04]\n",
            "  [ 2.0457371e-03 -8.6569868e-04  5.1542907e-03 ...  4.9958681e-03\n",
            "   -1.7121035e-03  7.8855893e-03]]\n",
            "\n",
            " [[-5.0309522e-04  4.4534202e-03 -1.9659549e-03 ... -2.6231743e-03\n",
            "    5.7358244e-03  7.7906107e-03]\n",
            "  [ 7.3923417e-03  2.4798014e-03  8.5042202e-04 ... -1.1502475e-02\n",
            "    1.3583360e-03 -1.6984475e-03]\n",
            "  [ 1.0056873e-02  1.3161669e-03  9.1775222e-04 ... -9.2546679e-03\n",
            "    2.0896192e-03 -2.1223186e-03]\n",
            "  ...\n",
            "  [ 2.2501808e-04 -4.5149308e-03  3.1642434e-03 ...  7.2238212e-03\n",
            "    2.2181035e-03 -6.0627372e-03]\n",
            "  [ 6.0405029e-04 -7.8232912e-03  1.9879702e-03 ...  3.0079302e-03\n",
            "   -2.6432395e-03 -6.1211772e-03]\n",
            "  [-7.5822941e-04 -7.8953952e-03 -2.0419653e-03 ...  1.0313559e-03\n",
            "   -7.3179621e-03 -4.0565622e-03]]\n",
            "\n",
            " [[ 5.2614426e-03 -3.6515419e-03  5.9722536e-03 ...  2.5342635e-03\n",
            "   -5.7014357e-03  7.5924001e-04]\n",
            "  [ 8.3546331e-03 -4.6323678e-03  6.6119004e-03 ...  2.3543136e-03\n",
            "   -6.3373093e-03  2.2140263e-04]\n",
            "  [ 3.7256670e-03 -5.8770166e-03  3.8556920e-03 ...  5.3378390e-03\n",
            "    3.8048672e-03 -1.2673395e-03]\n",
            "  ...\n",
            "  [-1.2856242e-03 -1.5594807e-02  6.6429540e-03 ...  3.8781350e-03\n",
            "   -4.5862487e-03 -6.6545075e-03]\n",
            "  [ 3.3192225e-03 -1.5728315e-02  1.0989329e-02 ...  5.5783209e-03\n",
            "   -9.1881780e-03 -5.4492182e-03]\n",
            "  [-9.7275554e-04 -8.9489007e-03  8.8847065e-03 ...  6.4918031e-03\n",
            "   -8.6823748e-03 -7.9975231e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.3544055e-03  1.2333978e-03  2.9949667e-03 ... -2.5811311e-04\n",
            "   -5.1444760e-03 -3.2490326e-04]\n",
            "  [-2.5240700e-03  2.2364785e-03  5.7278848e-03 ...  5.4571772e-04\n",
            "   -8.9079412e-03 -1.6352163e-03]\n",
            "  [-2.2456753e-03 -2.9504616e-03  5.2667763e-03 ... -5.1250518e-04\n",
            "   -1.0538000e-02 -4.6570334e-03]\n",
            "  ...\n",
            "  [ 5.4359967e-03 -4.6113553e-03  8.8202469e-03 ...  9.4291038e-04\n",
            "   -4.8352564e-03  3.6454725e-03]\n",
            "  [-2.1370132e-03 -8.8778911e-03  1.1756645e-02 ...  5.3912946e-03\n",
            "   -1.0548787e-02  5.0439597e-03]\n",
            "  [ 4.9656942e-03 -9.0233525e-03  8.5822567e-03 ...  9.1571407e-03\n",
            "   -5.6466032e-03  5.8399369e-03]]\n",
            "\n",
            " [[ 5.5245864e-03 -2.0633996e-03  2.2499301e-03 ... -1.3006602e-04\n",
            "   -2.7240608e-03 -7.3924183e-04]\n",
            "  [ 1.0504943e-02 -5.3073284e-03  7.2920034e-03 ...  1.8696041e-03\n",
            "   -7.4935006e-03 -2.3751682e-05]\n",
            "  [ 5.6946813e-03 -7.4505521e-04  1.1625755e-02 ...  6.9733788e-03\n",
            "   -6.7695235e-03 -1.6093747e-04]\n",
            "  ...\n",
            "  [-7.2089378e-03 -5.6345304e-03  1.1210149e-02 ...  1.4600828e-02\n",
            "   -7.5406358e-03 -1.8986644e-03]\n",
            "  [-6.2224315e-03 -9.5817039e-04  1.4169538e-02 ...  1.8050475e-02\n",
            "   -7.4231164e-03 -1.6280857e-03]\n",
            "  [ 1.1068572e-03 -4.9229166e-03  1.5976563e-02 ...  1.7532879e-02\n",
            "   -1.1137889e-02 -5.8126607e-04]]\n",
            "\n",
            " [[ 2.4063764e-03  5.7691177e-03  1.2960843e-03 ... -4.0205545e-03\n",
            "    2.4318779e-03 -2.1764957e-03]\n",
            "  [ 2.0219593e-03  7.6686163e-03  1.2115138e-03 ...  1.3490851e-03\n",
            "    4.9897498e-03 -7.8888349e-03]\n",
            "  [-2.3444745e-04  8.0967546e-03  1.2717227e-03 ...  2.8433728e-03\n",
            "    3.6524236e-03 -4.6381778e-03]\n",
            "  ...\n",
            "  [-3.5972390e-03 -1.1846530e-02  3.4018671e-03 ...  2.0721559e-03\n",
            "   -1.1741169e-02 -8.8013377e-04]\n",
            "  [ 2.2161084e-03 -1.2475005e-02  8.7437574e-03 ...  3.9855023e-03\n",
            "   -1.5191230e-02  4.2973814e-04]\n",
            "  [ 4.9150242e-03 -8.3040241e-03  5.5133919e-03 ... -7.8815938e-05\n",
            "   -1.1446694e-02  6.3793003e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u39XGi-VgS4i",
        "outputId": "9d68d539-dd8e-4b66-f430-99c50d3c0bcf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-1.8967954e-04 -4.8379302e-03  4.4216500e-05 ... -2.6844705e-03\n",
            "  -3.8732965e-03 -1.5947804e-03]\n",
            " [ 3.5951680e-03 -4.5101284e-03 -1.0560903e-03 ... -2.5638705e-03\n",
            "  -6.3740410e-04 -2.1776652e-03]\n",
            " [ 6.3097887e-03 -3.9878706e-03 -6.6788640e-04 ... -8.4625802e-04\n",
            "   1.4776374e-03  5.7361205e-03]\n",
            " ...\n",
            " [-1.0655216e-03 -1.1174589e-02  1.1930048e-02 ...  7.6719783e-03\n",
            "  -1.3978775e-02  3.3099328e-03]\n",
            " [ 2.5860486e-03 -7.2288392e-03  8.2859602e-03 ...  8.6672632e-03\n",
            "  -1.0147412e-02  7.6457328e-04]\n",
            " [ 2.0457371e-03 -8.6569868e-04  5.1542907e-03 ...  4.9958681e-03\n",
            "  -1.7121035e-03  7.8855893e-03]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally we'll look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probability of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QyoNBNq5hFHR",
        "outputId": "01279ae1-ade4-4e7f-eae6-0428588461a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.8967954e-04 -4.8379302e-03  4.4216500e-05 -1.7776679e-03\n",
            "  4.8225066e-03 -2.3276508e-03 -5.0586776e-04  6.5414347e-03\n",
            " -6.1750491e-03  4.0324097e-03 -7.0799775e-03  8.9374674e-04\n",
            " -1.5559488e-03 -4.5747254e-03 -4.5672851e-03  4.5846548e-04\n",
            " -6.9041173e-03  9.2828792e-04  9.6362166e-04  4.4138907e-03\n",
            " -3.6721840e-03  2.6349446e-03 -3.5190273e-03 -5.1060515e-03\n",
            "  6.6353788e-04  7.2255940e-04 -1.6945989e-03 -6.6888463e-03\n",
            " -8.0346977e-03  2.7390618e-03 -1.2528415e-04 -8.7365072e-04\n",
            "  9.0455054e-04  2.0543049e-04  2.6783173e-03  6.9034990e-04\n",
            " -4.1095726e-03  1.8450888e-03  2.5799291e-03 -9.4007549e-04\n",
            "  3.3457570e-03 -6.8186629e-03 -5.3894514e-04 -2.5526199e-03\n",
            " -2.9710587e-03 -6.2200014e-04  7.3780078e-03  2.7948746e-04\n",
            " -3.0212307e-03  3.7137431e-04  2.3847180e-03  8.4244367e-04\n",
            " -1.3813739e-04  6.7474874e-04  5.3491215e-03  2.5765444e-03\n",
            "  3.0667270e-03 -4.2434805e-03 -3.2067641e-03 -3.2252558e-03\n",
            " -1.9770078e-03 -1.0076161e-03 -2.6844705e-03 -3.8732965e-03\n",
            " -1.5947804e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tNqNGQ_miJwS",
        "outputId": "ab7aa9b3-a8c0-4bbd-ddab-8d53ac16f647"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TsvBHf juRF!nERTGns VZCAwD-fYzZ-$fp-L-pbg$nCj&jnIewgJ!s?-Y sWFtwC&ICIwN,erMa&;mNHcgkeLkCwrVumjLNIb!a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> So now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were."
      ],
      "metadata": {
        "id": "cesCyv01i-3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "sOxmALBei6JS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the Model\n",
        "> Before training, the model must be compiled with an optimizer, loss function, and optional metrics. This configures how the model updates weights and evaluates performance during training."
      ],
      "metadata": {
        "id": "u-C9Vw9bjEIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "YBp0elCejDWr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Checkpoints\n",
        "> To prevent losing training progress and to resume training later, we use model checkpoints.  \n",
        "> A checkpoint saves the model's weights, optimizer state, and training progress after each epoch."
      ],
      "metadata": {
        "id": "6WL_TFe3oV2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "wnMuJx1FoQ59"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        ">With the model compiled and checkpoints set up, we can begin training.\n",
        "During each epoch, the model learns to predict the next character based on previous ones, updating weights to minimize loss."
      ],
      "metadata": {
        "id": "ML9luAZipmDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "hKqPVdyHpOTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c94f7022-fb1c-463e-f9d0-53bdf13bb27a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 2.8950\n",
            "Epoch 2/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - loss: 1.8906\n",
            "Epoch 3/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 1.6236\n",
            "Epoch 4/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - loss: 1.4909\n",
            "Epoch 5/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - loss: 1.4167\n",
            "Epoch 6/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.3657\n",
            "Epoch 7/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.3274\n",
            "Epoch 8/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 1.2911\n",
            "Epoch 9/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.2564\n",
            "Epoch 10/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 1.2307\n",
            "Epoch 11/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.2035\n",
            "Epoch 12/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.1715\n",
            "Epoch 13/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.1428\n",
            "Epoch 14/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.1139\n",
            "Epoch 15/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 1.0828\n",
            "Epoch 16/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 72ms/step - loss: 1.0502\n",
            "Epoch 17/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 1.0192\n",
            "Epoch 18/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - loss: 0.9862\n",
            "Epoch 19/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9537\n",
            "Epoch 20/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9178\n",
            "Epoch 21/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.8837\n",
            "Epoch 22/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.8503\n",
            "Epoch 23/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 0.8185\n",
            "Epoch 24/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.7881\n",
            "Epoch 25/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.7587\n",
            "Epoch 26/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.7324\n",
            "Epoch 27/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.7024\n",
            "Epoch 28/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 0.6796\n",
            "Epoch 29/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 72ms/step - loss: 0.6541\n",
            "Epoch 30/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 0.6326\n",
            "Epoch 31/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - loss: 0.6149\n",
            "Epoch 32/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5928\n",
            "Epoch 33/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5762\n",
            "Epoch 34/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5607\n",
            "Epoch 35/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5466\n",
            "Epoch 36/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - loss: 0.5312\n",
            "Epoch 37/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5205\n",
            "Epoch 38/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.5090\n",
            "Epoch 39/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4988\n",
            "Epoch 40/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4904\n",
            "Epoch 41/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4821\n",
            "Epoch 42/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4728\n",
            "Epoch 43/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4649\n",
            "Epoch 44/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4582\n",
            "Epoch 45/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4520\n",
            "Epoch 46/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4484\n",
            "Epoch 47/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 0.4413\n",
            "Epoch 48/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4373\n",
            "Epoch 49/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4342\n",
            "Epoch 50/50\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.4284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model\n",
        "\n",
        "> Before training or generating text, we need to load the pre-defined RNN model and move it to the GPU for faster computation. Ensure that the model architecture matches the saved weights if loading from a checkpoint."
      ],
      "metadata": {
        "id": "3FfgUtRO1l_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "US5C321M1U37"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Load the latest trained weights from checkpoints and build the model with a flexible input shape for inference or further training."
      ],
      "metadata": {
        "id": "PI18OP_u6aOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the latest .weights.h5 file in the checkpoint directory\n",
        "weight_files = glob.glob(os.path.join(checkpoint_dir, \"*.weights.h5\"))\n",
        "if weight_files:\n",
        "    latest_weights = max(weight_files, key=os.path.getmtime)\n",
        "    model.load_weights(latest_weights)\n",
        "    print(f\"Loaded weights from {latest_weights}\")\n",
        "else:\n",
        "    print(\"No weight files found, starting from scratch.\")\n",
        "\n",
        "# Build the model with a flexible input shape\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dT7pa2T17Mc",
        "outputId": "d8c11875-e718-45c2-85a3-45931590d805"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights from ./training_checkpoints/ckpt_50.weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We can load **any checkpoint** we want by specifying the exact file to load."
      ],
      "metadata": {
        "id": "wvHArUvp9akt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_num = 10\n",
        "# checkpoint_path = f\"./training_checkpoints/ckpt_{checkpoint_num}.weights.h5\"\n",
        "\n",
        "# # Load weights directly (no tf.train.load_checkpoint)\n",
        "# model.load_weights(checkpoint_path)\n",
        "# print(f\"Loaded weights from {checkpoint_path}\")\n",
        "\n",
        "# # Build model with flexible input shape\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "hixe9WC36dRi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Recreate the model architecture (same as training)\n",
        "# model = build_model(\n",
        "#     vocab_size=len(vocab),\n",
        "#     embedding_dim=EMBEDDING_DIM,\n",
        "#     rnn_units=RNN_UNITS,\n",
        "#     batch_size=1\n",
        "# )\n",
        "\n",
        "# # Load the saved weights\n",
        "# checkpoint_num = 10\n",
        "# checkpoint_path = f\"./training_checkpoints/ckpt_{checkpoint_num}.weights.h5\"\n",
        "# model.load_weights(checkpoint_path)\n",
        "# print(f\"Loaded weights from {checkpoint_path}\")\n",
        "\n",
        "# # Build with flexible input shape\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "w3kpbyh248bQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text\n",
        "\n",
        "> This function uses the trained RNN model to generate text character by character.\n",
        ">   \n",
        "Starting from a seed string, it predicts the next character based on learned patterns and feeds it back into the model to continue generation.\n",
        ">   \n",
        "Adjusting the **temperature** value controls creativity—lower values make predictions more deterministic, while higher ones make the output more diverse."
      ],
      "metadata": {
        "id": "Ek72uR3M9rVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Numbers of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch_size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # We pass the predicted characters as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "fgdcnfd89HSA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 800\n",
        "\n",
        "    # Converting the start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty list to store the results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature → more predictable text\n",
        "    # High temperature → more creative / random text\n",
        "    temperature = 1.0\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        # Get predictions for the current input\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Adjust predictions by temperature\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample from the probability distribution\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Use the predicted character as the next input\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Append the predicted character to the result\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)"
      ],
      "metadata": {
        "id": "kCw6CO0n9n_A"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygMpyuQW3t_a",
        "outputId": "9cb82ceb-d0a7-452a-8038-91caa9113e6d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeowing all my heir not give\n",
            "multife:\n",
            "'Tis virtuous slave,\n",
            "A glorious angel on; fain be thou rages!\n",
            "\n",
            "ALONS:\n",
            "If my shephend hell unswept'd the coats will seem\n",
            "Of my purpose.'\n",
            "\n",
            "Nurse:\n",
            "A man, a goodly lady, and general,\n",
            "I have forgot your daughter, here 'tistance, which once untainted as I am, it\n",
            "therein the head to them. Come all to pieces.\n",
            "We villain, with old Ving Edward's guard!\n",
            "And kneel not what; I can.\n",
            "\n",
            "GREMIO:\n",
            "What's that?\n",
            "\n",
            "JULIET:\n",
            "'Tis to bed, with one profound, than might happy in\n",
            "him: his chair,\n",
            "And and then, to break an oath with some hour;\n",
            "Bid her daughter now to be avoided,\n",
            "Or else a holy man, and not my fare\n",
            "With brief wench'd and no sees grey so fast?\n",
            "\n",
            "Second Murderer:\n",
            "I pray thee, madam:\n",
            "To be it from thy beauty when they\n",
            "say, eyebrace of one direct I fear;\n",
            "But now the Duke of M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources\n",
        "\n",
        "1. Chollet, François. *Deep Learning with Python*. Manning Publications, 2018.  \n",
        "2. [**Text Classification with an RNN** - TensorFlow Core](https://www.tensorflow.org/tutorials/text/text_classification_rnn)  \n",
        "3. [**Text Generation with an RNN** - TensorFlow Core](https://www.tensorflow.org/tutorials/text/text_generation)  \n",
        "4. [**Understanding LSTM Networks** - Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
        "5. [**Text Generation with RNNs (TensorFlow & Keras)** - freeCodeCamp.org](https://www.youtube.com/watch?v=tPYj3fFJGjk)"
      ],
      "metadata": {
        "id": "IArfQNLE-ds4"
      }
    }
  ]
}